---
title: "results_2_rs"
author: "Elizabeth Williams"
date: "2024-08-15"
output: html_document
---

## Load Packages

```{r load packages, message = FALSE}
library(revtools)
library(synthesisr)
library(dplyr)
library(janitor)
library(tidyverse)
library(stringr)
library(tidytext)
library(widyr)
library(ggraph)
library(ggplot2)
library(maps)
library(igraph)
library(base)
library(stm)
library(wordcloud)
library(sf) #vector package
library(terra) #raster package
library(ggbreak)
library(UpSetR)
```

## Add Functions
```{r add functions}
convert_to_TF <- function(x) {
  return(grepl("TRUE", x, ignore.case = TRUE))
}

check_terms <- function(input_column, key_terms) {
  sapply(key_terms, function(term) grepl(term, input_column, ignore.case = TRUE)) %>% any()
}

```

## Import data
```{r load data, message = FALSE}
#import remote sensing papers
rs_data <- read.csv("data_working/rs_data.csv")

```


## Spatial & Temporal Extent
```{r spatial & temporal plot}
#Clean study extent (km2)
rs_data$study_scale <- suppressWarnings(as.numeric(rs_data$study_scale))

#Clean temporal scale (year range of imagery)
rs_data$year_start <- suppressWarnings(as.numeric(rs_data$year_start))
rs_data$year_end <- suppressWarnings(as.numeric(rs_data$year_end))

rs_data <- rs_data %>%
  mutate(year_range = year_end - year_start)

#Plot spatial v temporal scale

ggplot(rs_data, aes(x = study_scale, y = year_range)) +
  geom_point(size = 2) +
  labs(title = "Temporal & Spatial Scale of Remote Sensing Research",
       x = "Study extent (km2)",
       y = "Temporal scale (year range)") +
  theme_minimal() +
  #scale_x_break(c(50000, 100000)) +
  scale_x_continuous(trans='sqrt') +  #or log2 or log10
  theme(axis.text.x = element_text(angle = 45, hjust = 1))  # Rotate x-axis text labels

#HEXBIN plot or log transform the x axis
# the spatial and temporal domains of modern ecology reference

summary(rs_data$year_range)
summary(rs_data$study_scale)
```

## Imagery

make a spatial resolution barpolt after the first draft is written!

```{r imagery}

table(rs_data$imagery)

imagery_names <- c("landsat", "spot", "modis", "sentinel", "worldview", "aerial",
                   "rapideye", "planet", "photo", "quickbird", "lidar", "proba", 
                   "formosat", "irs-liss-ii", "hj-1")


imagery_counts <- map_dbl(imagery_names, function(name) {
  sum(str_detect(rs_data$imagery, fixed(name, ignore_case = TRUE)))
})

summary_imagery <- setNames(imagery_counts, imagery_names)

print(summary_imagery)

```
```{r}
# simplify imagery
groups_rs_imagery <- list(
  landsat = c("landsat"),
  spot = c("spot"),
  modis = c("modis"),
  sentinel = c("sentinel"),
  worldview = c("worldview"),
  aerial = c("aerial"),
  rapideye = c("rapideye"),
  planet = c("planet"),
  photo = c("photo"),
  quickbird = c("quickbird"),
  lidar = c("lidar"),
  proba = c("proba"),
  formosat = c("formosat"),
  irs_liss_ii = c("irs-liss-ii"),
  hj_1 = c("hj-1")
)

# sort property interaction results into the set groups
for (group in names(groups_rs_imagery)) {
  rs_data[[group]] <- sapply(rs_data$imagery, function(x) check_terms(x, groups_rs_imagery[[group]]))
}

# Select only the columns specified in the groups_field_prop list
imagery_rs_matrix <- rs_data %>%
  select(all_of(names(groups_rs_imagery))) %>%
  mutate(across(everything(), as.integer))  # TRUE/FALSE to 1/0

# Create UpSet plot
upset(imagery_rs_matrix, sets = c("landsat", "spot", "modis", "sentinel", "worldview", "aerial", "rapideye", 
      "planet", "photo", "quickbird", "lidar", "proba", "formosat", "irs_liss_ii", "hj_1"),
      mb.ratio = c(0.55, 0.45), order.by = "freq")

```

#Measurements & Prop Interactions

```{r cleaning rs measurements & property interactions}

#create temporary list of columns to be made to TRUE/FALSE
tmp_rs_convert <- c("veg_cover", "veg_ecotype", "veg_other", "int_veg_prop", "rs_model")

# make listed columns T/F
for (col in tmp_rs_convert) {
  rs_data[[paste0(col, "_TF")]] <- convert_to_TF(rs_data[[col]])
}

# group property interactions
groups_rs_prop <- list(
  prop_topo = c("slope", "aspect", "elevation", "curvature", "convergence index", 
                 "topographic wetness index", "TWI", "roughness"),
  prop_litho = c("lithology", "parent material", "substrate", "erodibility"),
  prop_weather = c("exposure", "solar", "temp", "rain", "precip", "wind", "snow"),
  prop_age = c("age"),
  prop_size = c("size", "perimeter", "area"),
  prop_veg = c("veg", "land use", "tree"),
  prop_disturb_prox = c("distance", "proximity", "association"),
  prop_zone = c("zone"),
  prop_nutrients = c("soil OC", "soil ph")
)

# sort property interaction results into the set groups
for (group in names(groups_rs_prop)) {
  rs_data[[group]] <- sapply(rs_data$int_veg_prop, function(x) check_terms(x, groups_rs_prop[[group]]))
}
```


Terms included in disturbance proximity properties: c("distance to infrastructure", "distance from edge", "distance to fault", 
                            "distance to road", "road association", "road proximity", 
                            "distance to active fault", "urban residential proximity", 
                            "proximity to stream", "distance to river", "riverbank proximity")

```{r upset chart for prop interactions}
# Select only the columns specified in the groups_field_prop list
prop_rs_matrix <- rs_data %>%
  select(all_of(names(groups_rs_prop))) %>%
  mutate(across(everything(), as.integer))  # TRUE/FALSE to 1/0

# Create UpSet plot
upset(prop_rs_matrix, sets = c("prop_topo", "prop_litho", "prop_weather", "prop_age", "prop_size", 
      "prop_veg", "prop_disturb_prox", "prop_zone", "prop_nutrients"), mb.ratio = c(0.55, 0.45), order.by = "freq")
```


## Cleaning Algorithms for Recovery Detection

split this into supervised & unsupervised after the first draft
this will require rereading the papers for if they use labelled data

```{r cleaning recovery algorithms}
# Algorithms for Recovery Detection

groups_algorithms <- list(
  deep_learning = c("neural network", "CNN", "MLP", "propagation"),
  statistical = c("tree", "random", "extreme", "neighbours", "svm", "linear", "regression", "bayes", "sarima", "casa", "pixel", "cube")
)

rs_data$rs_model <- ifelse(grepl("FALSE", rs_data$rs_model), "FALSE", rs_data$rs_model)

table(rs_data$rs_model)

for (group in names(groups_algorithms)) {
  rs_data[[group]] <- sapply(rs_data$rs_model, function(x) check_terms(x, groups_algorithms[[group]]))
}

rs_model_papers <- rs_data %>%
  filter(statistical == TRUE | deep_learning == TRUE)

```

#Cleaning ground truthing

```{r ground truthing}
# split ground truth into 3 categories

rs_data <- rs_data %>%
  mutate(ground_truth = case_when(
    grepl("partial", ground_truth, ignore.case = TRUE) ~ "partial",
    grepl("full", ground_truth, ignore.case = TRUE) ~ "full",
    TRUE ~ "FALSE"
  ))


```



# Printing RS results
```{r print results summary}
#Imagery
print(summary_imagery)


# Group TF columns
summary_rs_columns <- c("veg_cover_TF", "veg_ecotype_TF", "veg_other_TF", "int_veg_prop_TF", 
                     "rs_model_TF", "prop_topo", "prop_litho", "prop_weather", "prop_age", 
                     "prop_size", "prop_veg", "prop_disturb_prox", "prop_zone", 
                     "prop_nutrients", "deep_learning", "statistical")

# Extract relevant columns
rs_to_summarize <- rs_data[summary_rs_columns]

# Count TRUE and FALSE values across all specified columns
true_counts <- colSums(rs_to_summarize == "TRUE", na.rm = TRUE)
false_counts <- colSums(rs_to_summarize == "FALSE", na.rm = TRUE)

# Create a summary data frame
summary_rs_TF <- data.frame(
  True = true_counts,
  False = false_counts,
  stringsAsFactors = FALSE
)

# Display the final summary table
print(summary_rs_TF)


# Summarizing ground_truth
summary_ground_truth <- rs_data %>%
  group_by(ground_truth) %>%
  summarize(count = n())

print(summary_ground_truth)

```


